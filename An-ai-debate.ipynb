{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e70fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import permutations, product\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Tuple\n",
    "import os\n",
    "import sqlite3 \n",
    "load_dotenv('.env')\n",
    "max_attempt = 10\n",
    "# https://blog.google/technology/ai/google-gemini-ai/#performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93204faa",
   "metadata": {},
   "source": [
    "# AI vs AI: Letting LLM competing and judging themself\n",
    "\n",
    "OpenAI has been leading the way in AI development, expanding the horizons of machine capabilities. Their work on Large Language Models (LLMs) has been groundbreaking, allowing machines to produce human-like text, answer queries, translate languages, and even write code. With the advent of the short revolution, new LLMs have emerged, such as Mistral AI, Gemini, and Claude-3, often surpassing their predecessors in various fields. Their performance is evaluated through benchmarks like the Massive Multitasks Language Understanding (MMLU) designed to measure the knowledge and problem-solving abilities of language models across a wide range of subjects, from STEM fields to the humanities and social sciences; TriviaQA dataset consisting of a comprehensive reading comprehension dataset that contains over 650,000 question-answer-evidence triples; and many others.  However, these tests may not fully capture an AI's capabilities, as real-world scenarios are often more complex and interactive than benchmark tests.\n",
    "\n",
    "Unlike traditional metrics and tests used to evaluate LLMs, I was wondering whether we could assess AI by using AI: allowing AI models to interact directly with each other and evaluate their own responses and those of their peers. Inspired by debates used as a training framework to shape and challenge the minds of future leaders, lawyers, or judges (see e.g. [The importance of debate](https://oxfordsummercourses.com/articles/the-importance-of-debate/) or the [Debating society germany](https://www.schoolsdebate.de/index.php/about-us/our-competitions)), I designed a simple framework for AI to challenge each other. This framework consists of four parts: topic generation, debating, judging, and voting. In each part, roles will be assigned to LLMs using COSTAR prompts. The results format will be assessed, and if it doesn't fit the task, it will be rescheduled.\n",
    "\n",
    "The questions I aimed to explore are:\n",
    "\n",
    "- Do LLMs perform better in debates when they choose their own topics?\n",
    "- Are LLMs more effective in the proposing or opposing team?\n",
    "- Do they judge a debate impartially; i.e., do they not favor themselves?\n",
    "\n",
    "## Article goal:\n",
    "\n",
    "In this article we will see how an debate between AI models can be created and evaluated. For the evaluation of the debats\n",
    "\n",
    "## Short summary of LLM used\n",
    "**OpenAI - ChatGPT-4:** GPT-4, the latest model from OpenAI, showcases several significant improvements over its predecessor, GPT-3.5. Its key strengths include enhanced language understanding and generation, enabling it to comprehend and produce various dialects and emotional nuances, as well as create more coherent and creative content. GPT-4 also demonstrates superior reasoning and problem-solving capabilities, tackling complex mathematical and scientific problems with ease. Its multimodal capabilities set it apart, as it can analyze and comment on images and graphics. With an increased scale and capacity, GPT-4 caters to long-form content creation, extended conversations, and document analysis. Moreover, its safety and alignment have been improved, making it more reliable in providing factual responses and refusing disallowed content. Lastly, GPT-4's advanced programming abilities make it a valuable resource for software developers.\n",
    "\n",
    "Benchmarks: https://openai.com/research/gpt-4\n",
    "\n",
    "**MistralAI:** This model showcases exceptional capabilities, such as efficient processing through a sparse mixture of experts, dynamic expert utilization for nuanced responses, and the ability to handle a context of 32k tokens. MistralAI's emphasis on open technology leadership, strong financial backing, and commitment to efficiency in AI solutions further solidify its position in the AI market. Additionally, MistralAI offers a range of products tailored to different needs, from cost-effective endpoints like Mistral-tiny to more robust offerings like Mistral-medium.\n",
    "\n",
    "Benchmarks: https://docs.mistral.ai/platform/endpoints/\n",
    "\n",
    "**Gemini-Pro:** Gemini-Pro showcases several notable improvements and features compared to its predecessors. Its key strengths include enhanced performance and efficiency, achieving comparable quality to larger models while using fewer computational resources. The model introduces a breakthrough in long-context understanding, processing up to 1 million tokens, which is the longest among any large-scale foundation models. Gemini-Pro also boasts multimodal capabilities, supporting both text and image inputs and comprehending 38 languages. With a focus on safety and alignment, Google has conducted extensive ethics and safety testing for responsible deployment. Additionally, Gemini-Pro offers faster inference speed, potentially leading to real-time latency gains, and demonstrates strong abilities in following simple instructions.\n",
    "\n",
    "Benchmarks: https://blog.google/technology/ai/google-gemini-ai/#performance\n",
    "\n",
    "**Claude-3:** Claude-3, the AI model from Anthropic, demonstrates more natural, human-like language abilities, engaging in coherent, creative, and nuanced conversations. The model outperforms competitors like GPT-4 in IQ tests and excels in mathematics, information retrieval, and other benchmarks. Claude-3 also features multimodal capabilities, enabling it to analyze and comment on images and graphics. nthropic has prioritized safety and alignment, making Claude-3 more reliable and less prone to harmful outputs. Lastly, the model's versatility allows it to tackle a wide range of tasks, from creative writing to academic-style analysis.\n",
    "\n",
    "Benchmarks: https://www.anthropic.com/news/claude-3-family\n",
    "\n",
    "# An Ai-debate\n",
    "\n",
    "## Pre-requisit\n",
    "\n",
    "I first created an API key for each of the platform (except for Gemini which is using gcloud authentification and Vertex AI api). I placed my keys in a local environment file, saved as:\n",
    "```\n",
    "OPENAI_API_KEY = '1234'\n",
    "MISTRAL_API_KEY = '5678'\n",
    "ANTHROPIC_API_KEY = '9101112'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_debater.models.openai_chatter import OpenAIChatter\n",
    "from ai_debater.models.gemini_chatter import GeminiChatter\n",
    "from ai_debater.models.mistralai_chatter import MistralAIChatter\n",
    "from ai_debater.models.abstractai_chatter import BaseAiChatter\n",
    "from ai_debater.models.claude3ai_chatter import Claude3AiChatter\n",
    "\n",
    "\n",
    "def generate_model(models2generate: List[str]) -> Tuple[List[BaseAiChatter], pd.DataFrame]:\n",
    "    models = list()\n",
    "    if 'GeminiChatter' in models2generate:\n",
    "        models.append(GeminiChatter())\n",
    "    if 'MistralAIChatter' in models2generate:\n",
    "        models.append(MistralAIChatter(api_key = os.environ.get('MISTRAL_API_KEY')))\n",
    "    if 'OpenAIChatter' in models2generate:\n",
    "        models.append(OpenAIChatter(api_key=os.environ.get('OPENAI_API_KEY')))\n",
    "    if 'Claude3AiChatter' in models2generate:\n",
    "        models.append(Claude3AiChatter(api_key=os.environ.get('ANTHROPIC_API_KEY')))\n",
    "    if not models:\n",
    "        raise NameError(\"No model has been created\")\n",
    "    model_infos = pd.DataFrame({m.model_id():m.metainfo for m in models}).transpose()\n",
    "    model_infos.index.name = 'model_id'\n",
    "    model_infos.columns.name = 'property'\n",
    "    model_infos = model_infos.reset_index()\n",
    "    return models, model_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8e3e7",
   "metadata": {},
   "source": [
    "We will store the results in a database to later easily retrieve and combine different informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2115db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your connection.\n",
    "from ai_debater.io_database import IODataBase\n",
    "result_manager = IODataBase('results/dataset_v2.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e0768",
   "metadata": {},
   "source": [
    "## Topics generation\n",
    "\n",
    "The source of a debate is a controversial topics; indeed when everyone agrees nothing needs to be discussed nor debated. I could have come up with different topics by myself, or used popular topics used in debates. However as I wanted to evaluate the performance of AI on their own generated view of potential challenges, I first let each AI generate 10 topics.\n",
    "\n",
    "A topic may be summarise in a one sentence. When designing the prompt I quickly realised that the topic may not have a given side or may be ambiguous. I therefore decided to not only let the LLM generate the topic but as well a rational of this topic. This generated slighlty more context for the debating AIs. \n",
    "\n",
    "For the model generation: OpenAI, Mistral-AI, and Gemini-AI has been used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a9447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_debater.prompt_engineering import TopicCreatorContext\n",
    "role = TopicCreatorContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_topics = result_manager.load_topics()\n",
    "models, model_infos = generate_model(['GeminiChatter','MistralAIChatter','OpenAIChatter','Claude3AiChatter'])\n",
    "if existing_topics is not None:\n",
    "    model_infos = model_infos.loc[model_infos.model_entity.isin(existing_topics.model_entity)==False]\n",
    "    models=[] if model_infos.empty else [models[a] for a in model_infos.index]\n",
    "\n",
    "topics = {}\n",
    "for model in tqdm(models):\n",
    "    model.initialise(role)\n",
    "    topics[model.model_id()] = model.answer_until_valid([])\n",
    "\n",
    "# SavingOpenAIChatter\n",
    "if models:\n",
    "    topics = pd.concat(topics)\n",
    "    topics.index.names = ['model_id', topics.index.names[-1]]\n",
    "    topics = topics.reset_index()\n",
    "    topics['topic_id'] = topics['model_id'] + '-' + topics['ith_topic'].astype(str)\n",
    "    topics.to_sql(\"topics\", result_manager.connection, if_exists='append')\n",
    "    model_infos.to_sql(\"model_infos\", result_manager.connection, if_exists='append')\n",
    "existing_topics = result_manager.load_topics()\n",
    "existing_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f82e0",
   "metadata": {},
   "source": [
    "## Debate Setup\n",
    "\n",
    "A debate involves two teams: a proposing team and an opposing team. It typically runs for a specified duration, during which team members take turns presenting and refuting arguments from the other team. Given that long debates might be challenging for LLMs due to potential loss of context over multiple chat iterations, I chose to limit debates to 4 turns.\n",
    "\n",
    "To account for possible differences in LLM performance based on team role, I decided to have each pair of models participate as both the proposing and opposing team for a given topic. With four AI models and 30 topics, this results in 360 possible debates. To keep the scope manageable, I limited the debates to two scenarios:\n",
    "\n",
    "1. OpenAI vs Mistral AI\n",
    "2. Mistral AI vs Claude-3 AI\n",
    "\n",
    "### Data Modeling\n",
    "\n",
    "To retain essential information for later analysis, we want to store:\n",
    "\n",
    "- The topic, identified by the topic-id\n",
    "- The proposing model, identified by the model-id\n",
    "- The opposing model, identified by the model-id\n",
    "- The argument, argument position, and the model speaking\n",
    "\n",
    "The data schema consists of two tables:\n",
    "\n",
    "- fact_discourse:\n",
    "    - Argument, ith_argument, foreign_key_speaking, foreign_key_id\n",
    "    - Unique key: argument_id\n",
    "- dim_discourse:\n",
    "    - Unique key: discourse_id\n",
    "    - model_proposing, model_opposing, topic_id\n",
    "\n",
    "### OpenAI vs Mistral AI Debate Setup\n",
    "\n",
    "For the first debate, I set up a competition between OpenAI and Mistral AI, following the guidelines and data modeling described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c444d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_debater.debater_tools import run_debate\n",
    "from ai_debater.prompt_engineering import DebaterContext\n",
    "role = DebaterContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab75f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_in_competitions = ['MistralAIChatter','OpenAIChatter']\n",
    "models, model_infos = generate_model(model_in_competitions)\n",
    "need2save = True\n",
    "for model in tqdm(models):\n",
    "    model.initialise(role)\n",
    "existing_competitions = result_manager.load_competitions()\n",
    "existing_topics = result_manager.load_topics()\n",
    "topics_creators = [\n",
    "    'GeminiChatter|gemini-pro', # The neutral LLM\n",
    "    'MistralAIChatter|mistral-large-latest',\n",
    "    'OpenAIChatter|gpt-4'\n",
    "    ]\n",
    "selected_topics = existing_topics.loc[existing_topics.model_entity.isin(topics_creators)]\n",
    "\n",
    "competing_models = permutations(models, 2)\n",
    "for topic_index, (prop, oppo) in tqdm(product(selected_topics.index, competing_models)):\n",
    "    topic = existing_topics.loc[topic_index]\n",
    "    current_topic_id = topic.topic_id\n",
    "    competition_done = False\n",
    "    if existing_competitions is not None:\n",
    "        competition_done = existing_competitions.topic_id == current_topic_id\n",
    "        competition_done&= existing_competitions.model_proposing_entity == prop.model_entity\n",
    "        competition_done&= existing_competitions.model_opposing_entity == oppo.model_entity\n",
    "        competition_done = competition_done.any()\n",
    "    if competition_done:\n",
    "        continue\n",
    "    if need2save:\n",
    "        model_infos.to_sql(\"model_infos\", result_manager.connection, if_exists='append')\n",
    "        need2save = False\n",
    "    run_debate(topic, prop, oppo, result_manager.connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d8a0d",
   "metadata": {},
   "source": [
    "### Mistral AI vs Claude3 AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d4a510",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = DebaterContext()\n",
    "models, model_infos = generate_model(['MistralAIChatter','Claude3AiChatter'])\n",
    "need2save = True\n",
    "for model in tqdm(models):\n",
    "    model.initialise(role)\n",
    "existing_competitions = result_manager.load_competitions()\n",
    "existing_topics = result_manager.load_topics()\n",
    "topics_creators = [\n",
    "    'GeminiChatter|gemini-pro', # The neutral LLM\n",
    "    'MistralAIChatter|mistral-large-latest',\n",
    "    'Claude3AiChatter|claude-3-opus-20240229'\n",
    "    ]\n",
    "selected_topics = existing_topics.loc[existing_topics.model_entity.isin(topics_creators)]\n",
    "selected_topics = selected_topics.iloc[::-1]\n",
    "\n",
    "competing_models = permutations(models, 2)\n",
    "for topic_index, (prop, oppo) in tqdm(product(selected_topics.index, competing_models)):\n",
    "    topic = existing_topics.loc[topic_index]\n",
    "    current_topic_id = topic.topic_id\n",
    "    competition_done = False\n",
    "    if existing_competitions is not None:\n",
    "        competition_done = existing_competitions.topic_id == current_topic_id\n",
    "        competition_done&= existing_competitions.model_proposing_entity == prop.model_entity\n",
    "        competition_done&= existing_competitions.model_opposing_entity == oppo.model_entity\n",
    "        competition_done = competition_done.any()\n",
    "    if competition_done:\n",
    "        continue\n",
    "    if need2save:\n",
    "        model_infos.to_sql(\"model_infos\", result_manager.connection, if_exists='append')\n",
    "        need2save = False\n",
    "    run_debate(topic, prop, oppo, result_manager.connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270a9aaa",
   "metadata": {},
   "source": [
    "## Evaluation by the Jury\n",
    "\n",
    "Once the debate has concluded, it's necessary to assess the quality of the teams' responses and their roles in the debate. Debates can be evaluated based on various categories, such as Reasoning and Evidence, Listening and Response, Organisation and Prioritisation, Expression and Delivery, and Teamwork and Roles. Each judge will be responsible for grading the participants in each category and providing a rationale for their grade. This rationale is crucial for comparing different judgments on the same debate.\n",
    "\n",
    "Later, we will explore the concept of allowing LLMs to determine which judgment is most appropriate for the debate. This will help us evaluate whether AI models can convince other AI models that their judgment is the best.\n",
    "\n",
    "### Data Modeling for Judgments:\n",
    "\n",
    "To analyze the judgments later, we need to capture the following information:\n",
    "\n",
    "- The debate being judged: discourse ID\n",
    "- The model responsible for judging: model ID\n",
    "- The resulting judgments:\n",
    "    - Category, score, team, and rationale\n",
    "\n",
    "We will store this information in the following tables:\n",
    "\n",
    "- fact_judgments:\n",
    "    - Score, categories, team ID, and rationale\n",
    "    - An ID to identify the judgment: judgment_id\n",
    "- dim_judgments:\n",
    "    - Judgment_id: a unique key\n",
    "    - Discourse_id and model_id_judging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97368bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_debater.prompt_interface import discourse2input\n",
    "from ai_debater.prompt_engineering import JudgesContext\n",
    "role = JudgesContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models, model_infos = generate_model(['GeminiChatter','MistralAIChatter','OpenAIChatter', 'Claude3AiChatter']) # '\n",
    "need2save = True\n",
    "for model in tqdm(models):\n",
    "    model.initialise(role)\n",
    "\n",
    "enriched_judgements = result_manager.load_enriched_judgements()\n",
    "existing_competitions = result_manager.load_competitions()\n",
    "\n",
    "with tqdm(total=len(models)*existing_competitions.shape[0]) as pbar:\n",
    "    for _, competition in tqdm(existing_competitions.iterrows(), \"Judging competitions\", total=existing_competitions.shape[0]):\n",
    "        discourse_id = competition.discourse_id\n",
    "        discourse = result_manager.load_discourse(discourse_id)\n",
    "        message = discourse2input(discourse)\n",
    "        for judge in models:\n",
    "            model_id_judging = judge.model_id()\n",
    "            judgement_id = model_id_judging+':'+discourse_id\n",
    "            if enriched_judgements is not None:\n",
    "                condition = enriched_judgements.discourse_id == discourse_id\n",
    "                condition&= enriched_judgements.judge_model_entity == judge.model_entity\n",
    "                condition = condition.any()\n",
    "                if condition:\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "            result = judge.answer_until_valid([message])\n",
    "            if need2save:\n",
    "                model_infos.to_sql(\"model_infos\", result_manager.connection, if_exists='append')\n",
    "                need2save = False\n",
    "            fact_judgements = result.copy().reset_index()\n",
    "            fact_judgements['judgement_id'] = judgement_id\n",
    "\n",
    "            dim_judgements = pd.Series({'judgement_id': judgement_id,\n",
    "                                        'discourse_id': discourse_id,\n",
    "                                        'model_id_judging': model_id_judging})\n",
    "            \n",
    "            dim_judgements.to_frame().transpose().to_sql(\"dim_judgements\", result_manager.connection, if_exists='append')\n",
    "            fact_judgements.to_sql(\"fact_judgements\", result_manager.connection, if_exists='append')\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d12ca0",
   "metadata": {},
   "source": [
    "## Public\n",
    "\n",
    "In a open debate, the public sometimes to rate the audience. Often done with an applaumeters or counting raised hands, at the core we have the audience rating the participant. They may have heard the verdict of the jury prior to making their decision, and are therefore agreeing or disagreeing with a given judge rating. The agreement with a judgement is interesting in this AI debating scenarios as the an AI will judge and them based on its own judgement as well the judgement of others will need to decide which judgement is the most appropriate, i.e. changing its mind or keeping it line of reasoning. To avoid for an obvious link, by using the model name, the judge identity and the public identity are both unknown to the model. \n",
    "\n",
    "### Data Modeling\n",
    "\n",
    "To retain essential information for later analysis, we want to store:\n",
    "\n",
    "- The dicourse-id,\n",
    "- The judegement ids: here we have one per judges, so four\n",
    "- The id of the model acting as the public: public_id\n",
    "\n",
    "The data schema consists of two tables:\n",
    "\n",
    "- fact_public:\n",
    "    - judgement_id,\n",
    "    - public_voting_id,\n",
    "\n",
    "- dim_public:\n",
    "    - Unique key: public_voting_id\n",
    "    - discourse_id,\n",
    "    - public_model_id,\n",
    "    - judgement_ids: an Array of judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca6fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_debater.prompt_interface import judgement_and_discourse2input\n",
    "from ai_debater.prompt_engineering import PublicContext\n",
    "\n",
    "role = PublicContext()\n",
    "\n",
    "judgements = result_manager.load_judgements()\n",
    "judged_discourses = judgements.discourse_id.unique()\n",
    "public_votes = result_manager.load_enriched_public()\n",
    "\n",
    "models, model_infos = generate_model(['GeminiChatter','MistralAIChatter','OpenAIChatter', 'Claude3AiChatter'])\n",
    "need2save = True\n",
    "for model in tqdm(models):\n",
    "    model.initialise(role)\n",
    "for discourse_id in tqdm(judged_discourses):\n",
    "    message, judgement_ids = judgement_and_discourse2input(discourse_id=discourse_id, result_manager=result_manager)\n",
    "    messages = [message]\n",
    "    # I needed to add this, because MistralAI and OpenAI failed to respond with the correct output. \n",
    "    messages.append({'role':'system', 'content': 'thank you for providing the information. In which format should I answer?'})\n",
    "    messages.append({'role':'user', 'content': 'Please give the judgement id as: <Judgement_ID></Judgement_ID>'})\n",
    "    for model in models:\n",
    "        public_model_id = model.model_id()\n",
    "        public_voting_id = discourse_id+'|'+public_model_id\n",
    "        if public_votes is not None:\n",
    "            condition = public_votes.discourse_id == discourse_id\n",
    "            condition&= public_votes.public_model_entity == model.model_entity\n",
    "            condition = condition.any()\n",
    "            if condition:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "        \n",
    "        res = model.answer_until_valid(messages)\n",
    "        \n",
    "        if need2save:\n",
    "            model_infos.to_sql(\"model_infos\", result_manager.connection, if_exists='append')\n",
    "            need2save = False\n",
    "        fact_public = res.to_frame().transpose()\n",
    "        fact_public['public_voting_id'] = public_voting_id\n",
    "        dim_public = pd.Series({'public_voting_id': public_voting_id,\n",
    "                                'discourse_id': discourse_id,\n",
    "                                'public_model_id': public_model_id,\n",
    "                                'judgement_ids': judgement_ids})\n",
    "\n",
    "        dim_public.to_frame().transpose().to_sql(\"dim_public\", result_manager.connection, if_exists='append')\n",
    "        fact_public.to_sql(\"fact_public\", result_manager.connection, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed526039",
   "metadata": {},
   "source": [
    "## Data modelling summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b32f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "SVG(filename='AiDebate.drawio.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2138154",
   "metadata": {},
   "source": [
    "# Further reading:\n",
    "## Pricing\n",
    "\n",
    "* https://docs.mistral.ai/platform/pricing/\n",
    "* https://openai.com/pricing\n",
    "* https://cloud.google.com/vertex-ai/generative-ai/pricing\n",
    "\n",
    "## Interesting links\n",
    "\n",
    "* Gemini vs OpenAI:\n",
    "    - https://medium.com/@csakash03/googles-gemini-vs-openai-s-chatgpt-4d23288f7769\n",
    "    - https://www.thepromptindex.com/evaluating-googles-new-gemini-language-model-how-does-it-stack-up-against-gpt-3-and-gpt-4.html\n",
    "* Claude 3v vs Chat GPT:\n",
    "    - https://www.analyticsvidhya.com/blog/2024/03/claude-3-sonnet-vs-chatgpt-3-5/\n",
    "* How-Tos - Vertex AI:\n",
    "    - https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.generative_models.GenerativeModel#vertexai_generative_models_GenerativeModel_start_chat\n",
    "    - https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.generative_models.ChatSession\n",
    "    - Setting vertexai up: https://cloud.google.com/vertex-ai/docs/start/cloud-environment\n",
    "* How-Tos - Claude-3\n",
    "    - https://docs.anthropic.com/claude/docs/intro-to-claude\n",
    "* How-Tos - OpenAi:\n",
    "    - https://platform.openai.com/docs/introduction\n",
    "* How-Tos - MistralAI:\n",
    "    - https://docs.mistral.ai/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mydownloader-z0Y3U4OY-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
